{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZG6Zy44D+VcCz8bdJPF4Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mayur-666/Computer_Vision/blob/main/Assignment_3_Decision_Tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import tree\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "XOG2pR-Aa9_n"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "# Define the dataset\n",
        "data = [\n",
        "    [30, 'high', 'no', 'fair', 'no'],\n",
        "    [30, 'high', 'no', 'excellent', 'no'],\n",
        "    [31, 'medium', 'no', 'fair', 'yes'],\n",
        "    [40, 'low', 'no', 'fair', 'yes'],\n",
        "    [40, 'low', 'yes', 'fair', 'yes'],\n",
        "    [40, 'low', 'yes', 'excellent', 'no'],\n",
        "    [31, 'medium', 'yes', 'excellent', 'yes'],\n",
        "    [30, 'high', 'no', 'fair', 'no'],\n",
        "    [30, 'medium', 'yes', 'fair', 'yes'],\n",
        "    [31, 'medium', 'yes', 'excellent', 'yes'],\n",
        "    [31, 'high', 'no', 'excellent', 'yes'],\n",
        "    [40, 'medium', 'no', 'fair', 'yes'],\n",
        "    [40, 'high', 'yes', 'fair', 'yes'],\n",
        "    [31, 'medium', 'no', 'excellent', 'no']\n",
        "]\n",
        "\n",
        "# Convert the dataset into a DataFrame\n",
        "df = pd.DataFrame(data, columns=['age', 'income', 'student', 'credit_rating', 'buys_computer'])\n",
        "\n",
        "# Define the features and target variable\n",
        "X = df.drop(columns=['buys_computer'])\n",
        "y = df['buys_computer']\n",
        "\n",
        "# Define a class for the decision tree node\n",
        "class DecisionTreeNode:\n",
        "    def __init__(self, feature=None, value=None, left=None, right=None, target_class=None):\n",
        "        self.feature = feature  # Index of feature to split on\n",
        "        self.value = value  # Value of the feature\n",
        "        self.left = left  # Left subtree\n",
        "        self.right = right  # Right subtree\n",
        "        self.target_class = target_class  # Target class if the node is a leaf\n",
        "\n",
        "# Define a function to calculate entropy\n",
        "def calculate_entropy(y):\n",
        "    class_counts = y.value_counts()\n",
        "    entropy = 0\n",
        "    for count in class_counts:\n",
        "        probability = count / len(y)\n",
        "        entropy -= probability * math.log2(probability)\n",
        "    return entropy\n",
        "\n",
        "# Define a function to calculate information gain for a particular feature\n",
        "def calculate_information_gain(X, y, feature, split_value):\n",
        "    total_entropy = calculate_entropy(y)\n",
        "\n",
        "    # Split the dataset based on the feature and value\n",
        "    left_indices = X[feature] <= split_value\n",
        "    right_indices = X[feature] > split_value\n",
        "    left_entropy = calculate_entropy(y[left_indices])\n",
        "    right_entropy = calculate_entropy(y[right_indices])\n",
        "\n",
        "    # Calculate the information gain\n",
        "    left_weight = sum(left_indices) / len(y)\n",
        "    right_weight = sum(right_indices) / len(y)\n",
        "    information_gain = total_entropy - (left_weight * left_entropy + right_weight * right_entropy)\n",
        "\n",
        "    return information_gain\n",
        "\n",
        "# Define a function to build the decision tree recursively\n",
        "def build_decision_tree(X, y):\n",
        "    if len(set(y)) == 1:  # If all samples have the same class\n",
        "        return DecisionTreeNode(target_class=y.iloc[0])\n",
        "\n",
        "    if len(X.columns) == 0:  # If there are no features left to split on\n",
        "        return DecisionTreeNode(target_class=y.mode()[0])\n",
        "\n",
        "    best_information_gain = 0\n",
        "    best_feature = None\n",
        "    best_split_value = None\n",
        "\n",
        "    # Find the best feature and split value\n",
        "    for feature in X.columns:\n",
        "        unique_values = X[feature].unique()\n",
        "        for value in unique_values:\n",
        "            information_gain = calculate_information_gain(X, y, feature, value)\n",
        "            if information_gain > best_information_gain:\n",
        "                best_information_gain = information_gain\n",
        "                best_feature = feature\n",
        "                best_split_value = value\n",
        "\n",
        "    # Split the dataset based on the best feature and split value\n",
        "    left_indices = X[best_feature] <= best_split_value\n",
        "    right_indices = X[best_feature] > best_split_value\n",
        "    left_subtree = build_decision_tree(X[left_indices], y[left_indices])\n",
        "    right_subtree = build_decision_tree(X[right_indices], y[right_indices])\n",
        "\n",
        "    return DecisionTreeNode(feature=best_feature, value=best_split_value, left=left_subtree, right=right_subtree)\n",
        "\n",
        "# Build the decision tree\n",
        "decision_tree = build_decision_tree(X, y)\n",
        "\n",
        "# Define a function to make predictions using the decision tree\n",
        "def predict(tree, sample):\n",
        "    if tree.target_class is not None:\n",
        "        return tree.target_class\n",
        "    feature_index = X.columns.get_loc(tree.feature)\n",
        "    if sample[feature_index] <= tree.value:\n",
        "        return predict(tree.left, sample)\n",
        "    else:\n",
        "        return predict(tree.right, sample)\n",
        "\n",
        "# Make predictions for new samples\n",
        "data2 = [\n",
        "    [20, 'low', 'yes', 'excellent'],\n",
        "    [30, 'high', 'no', 'fair'],\n",
        "    [40, 'medium', 'yes', 'fair'],\n",
        "    [50, 'low', 'no', 'fair'],\n",
        "    [25, 'high', 'yes', 'excellent'],\n",
        "    [35, 'medium', 'no', 'fair'],\n",
        "    [45, 'low', 'yes', 'excellent'],\n",
        "    [55, 'high', 'no', 'fair'],\n",
        "]\n",
        "\n",
        "for sample in data2:\n",
        "    prediction = predict(decision_tree, sample)\n",
        "    print(f\"For sample {sample}, predicted class: {prediction}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVJujyl_rGxu",
        "outputId": "9cdab9bb-8c7f-4177-d55e-68265d05cb3b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For sample [20, 'low', 'yes', 'excellent'], predicted class: yes\n",
            "For sample [30, 'high', 'no', 'fair'], predicted class: no\n",
            "For sample [40, 'medium', 'yes', 'fair'], predicted class: yes\n",
            "For sample [50, 'low', 'no', 'fair'], predicted class: yes\n",
            "For sample [25, 'high', 'yes', 'excellent'], predicted class: no\n",
            "For sample [35, 'medium', 'no', 'fair'], predicted class: yes\n",
            "For sample [45, 'low', 'yes', 'excellent'], predicted class: no\n",
            "For sample [55, 'high', 'no', 'fair'], predicted class: yes\n"
          ]
        }
      ]
    }
  ]
}